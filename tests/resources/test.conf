max_epochs: 2

log_interval: 10

# refer to https://pytorch.org/docs/stable/tensor_attributes.html?highlight=memory_format#torch.torch.memory_format
# highly recommend use 'channels_last' on NVIDIA Tasla A100, V100 and RTX 3090 with typical CNNs
memory_format: "channels_last" # select from 'contiguous_format' | 'channels_last' | 'preserve_format'

use_amp: false # if true, it will train in automatic mixed precision mode

only_evaluate: false # if true, it will only evalute the model on the validation set and exit

sync_batchnorm: false # if true, it will convert all the batchnorm layers into torch.nn.SyncBatchNorm

accmulated_steps: 1

set_reproducible: false # if true, the training will be set to reproducible (refer to https://pytorch.org/docs/stable/notes/randomness.html)
                        # else torch.backends.cudnn.benchmark will be set to True for largest throughput

data {
    type_: synthetic_data

    image_size: 224
    batch_size: 16

    input_size: [16,3,224,224]
    target_size: [16]
    device: cpu
    length: 128

    num_classes: 1000
}

model {
    type_: dummy_model
    load_from: null
}

optimizer {
    type_: CustomSGD
    lr: 0.256
    basic_bs: 256 # we set lr=0.256 for 256 batch size, for other batch sizes we linearly scale the learning rate.
    momentum: 0.875
    dampening: 0
    weight_decay: 3.0517578125e-05 # refer to https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/ConvNets/resnet50v1.5
    nesterov: false
    bn_weight_decay: 0
}

scheduler {
    type_: WarmupCosineAnnealingLR
    T_warmup: 5
    T_max: ${max_epochs}
    eta_min: 0
}

criterion {
    type_: CrossEntropyLoss
}
